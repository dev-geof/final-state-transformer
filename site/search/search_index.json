{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FINAL STATE TRANSFORMER Final State Transformer (FST) is a machine learning development toolkit built upon Transformer encoder network architectures and specifically crafted for high-energy physics applications. Leveraging the power of the multi-head attention mechanism for capturing long-range dependencies and contextual information in sequences of particle-collision event final-state objects, it allows the design of machine learning models that excel in classification and regression tasks. Featuring a user-friendly interface, this toolkit facilitates the integration of Transformer networks into research workflows, enabling users to harness state-of-the-art machine learning techniques. Powered by:","title":"Home"},{"location":"#final-state-transformer","text":"Final State Transformer (FST) is a machine learning development toolkit built upon Transformer encoder network architectures and specifically crafted for high-energy physics applications. Leveraging the power of the multi-head attention mechanism for capturing long-range dependencies and contextual information in sequences of particle-collision event final-state objects, it allows the design of machine learning models that excel in classification and regression tasks. Featuring a user-friendly interface, this toolkit facilitates the integration of Transformer networks into research workflows, enabling users to harness state-of-the-art machine learning techniques. Powered by:","title":"FINAL STATE TRANSFORMER"},{"location":"code/","text":"The Code Source Code and Installation The source code for the FST toolkit is hosted on GitHub. Users can access the repository by visiting the final-state-transformer GitHub page. The repository contains all the scripts, utilities and documentation needed to understand and utilize the toolkit. To install the FST toolkit, follow the following steps to ensure smooth setup. Before installation, ensure you have the following software installed on your system: Python 3.9.13 or higher, Git, pip (Python package installer). Start by cloning the FST repository from GitHub to your local machine. Open your terminal and execute the following commands: git clone git@github.com:dev-geof/final-state-transformer.git cd final-state-transformer It is recommended to install the FST toolkit within a virtual environment. The python \"venv\" environment is a lightweight solution to install the necessary dependencies. python3 -m venv env source env/bin/activate python -m pip install -e . -r requirements.txt Software Implementation The FST software features a modular python architecture designed to facilitate the implementation, the training and the evaluation of Transformer encoder networks. The core structure is organized into three main tools and several utilities. Main Tools and Utilities The main tools provide essential functionalities for various stages of the machine learning pipeline. These stages include data preparation, model training, and evaluation. Data preparation tools ensure that raw data is cleaned, formatted, and features are engineered for optimal input into the model. Training tools encompass scripts and utilities for initiating and monitoring the training process, optimizing hyper- parameters, and supporting distributed training across CPUs or GPUs. Evaluation tools offer methods for calculating performance metrics, validating models, and interpreting model outputs. The utilities support the main tools and enhance their functionality. The Model utilities define and manage the architecture and parameters of Transformer-based models, including saving and loading models. Plotting utilities provide tools for visualizing training progress, model performance, and generating interactive plots. Data utilities handle efficient loading, batching, and management of datasets. Configu- ration utilities manage settings and parameters through configuration files, ensuring consistent experiment setups. Finally, Callbacks offer functions for logging progress, early stopping, and checkpointing models during training, improving the training process\u2019s efficiency and reliability. Entry Points The setup configuration specifies entry points for the tree main tool scripts, enabling users to execute data preparation, model training, and evaluation directly from the command line. This facilitates streamlined workflows and simplifies the execution of common tasks. The toolkit leverages YAML configuration files to allow users to pilot and manage all these scripts and tools efficiently. These configuration files store settings for data paths, hyper-parameters, training schedules, model configurations, and other experimen- tal parameters. The configuration system parses these files, ensuring that the entire pipeline\u2014from data preparation to model evaluation adheres to the specified settings. usage: fst-preparation [-h] [--configfile CONFIGFILE] fst-training [-h] [--configfile CONFIGFILE] fst-validation [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE YAML configuration file path Modularity The FST toolkit is designed with extensibility in mind, allowing users to easily adapt and extend the framework for different high-energy physics tasks. The modular design enables the integration of custom models, additional data processing techniques, and new evaluation metrics without altering the core structure of the toolkit.","title":"Code"},{"location":"code/#the-code","text":"","title":"The Code"},{"location":"code/#source-code-and-installation","text":"The source code for the FST toolkit is hosted on GitHub. Users can access the repository by visiting the final-state-transformer GitHub page. The repository contains all the scripts, utilities and documentation needed to understand and utilize the toolkit. To install the FST toolkit, follow the following steps to ensure smooth setup. Before installation, ensure you have the following software installed on your system: Python 3.9.13 or higher, Git, pip (Python package installer). Start by cloning the FST repository from GitHub to your local machine. Open your terminal and execute the following commands: git clone git@github.com:dev-geof/final-state-transformer.git cd final-state-transformer It is recommended to install the FST toolkit within a virtual environment. The python \"venv\" environment is a lightweight solution to install the necessary dependencies. python3 -m venv env source env/bin/activate python -m pip install -e . -r requirements.txt","title":"Source Code and Installation"},{"location":"code/#software-implementation","text":"The FST software features a modular python architecture designed to facilitate the implementation, the training and the evaluation of Transformer encoder networks. The core structure is organized into three main tools and several utilities.","title":"Software Implementation"},{"location":"code/#main-tools-and-utilities","text":"The main tools provide essential functionalities for various stages of the machine learning pipeline. These stages include data preparation, model training, and evaluation. Data preparation tools ensure that raw data is cleaned, formatted, and features are engineered for optimal input into the model. Training tools encompass scripts and utilities for initiating and monitoring the training process, optimizing hyper- parameters, and supporting distributed training across CPUs or GPUs. Evaluation tools offer methods for calculating performance metrics, validating models, and interpreting model outputs. The utilities support the main tools and enhance their functionality. The Model utilities define and manage the architecture and parameters of Transformer-based models, including saving and loading models. Plotting utilities provide tools for visualizing training progress, model performance, and generating interactive plots. Data utilities handle efficient loading, batching, and management of datasets. Configu- ration utilities manage settings and parameters through configuration files, ensuring consistent experiment setups. Finally, Callbacks offer functions for logging progress, early stopping, and checkpointing models during training, improving the training process\u2019s efficiency and reliability.","title":"Main Tools and Utilities"},{"location":"code/#entry-points","text":"The setup configuration specifies entry points for the tree main tool scripts, enabling users to execute data preparation, model training, and evaluation directly from the command line. This facilitates streamlined workflows and simplifies the execution of common tasks. The toolkit leverages YAML configuration files to allow users to pilot and manage all these scripts and tools efficiently. These configuration files store settings for data paths, hyper-parameters, training schedules, model configurations, and other experimen- tal parameters. The configuration system parses these files, ensuring that the entire pipeline\u2014from data preparation to model evaluation adheres to the specified settings. usage: fst-preparation [-h] [--configfile CONFIGFILE] fst-training [-h] [--configfile CONFIGFILE] fst-validation [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE YAML configuration file path","title":"Entry Points"},{"location":"code/#modularity","text":"The FST toolkit is designed with extensibility in mind, allowing users to easily adapt and extend the framework for different high-energy physics tasks. The modular design enables the integration of custom models, additional data processing techniques, and new evaluation metrics without altering the core structure of the toolkit.","title":"Modularity"},{"location":"configuration/","text":"Configuration The following lines rovides a comprehensive guide to the configuration file in YAML format used for the analysis. The file includes specifications for training and validation samples, input variables, general settings, data preparation parameters, validation configurations, and model hyperparameters. Each section is explained in detail to assist users in understanding and customizing the analysis settings. Training and Validation Samples training_samples : List of training samples, each identified by a label. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ] Color for visualization. type : [ str ] Type of the sample (signal or background). validation_samples : List of validation samples, similar to training samples. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ]Color for visualization. type : [ str ] Type of the sample (signal or background). Input Variables input_variables : List of final state particle features used in the analysis. ghost_variables : List of ghost event features to be included in prediction file for validation. General Configuration general_configuration : General settings for the analysis. output_directory : [ str ] Directory for storing output files. training_mode : [ str ] Accepted modes: \"classification\" or \"regression\". analysis_title : [ str ] Title for the analysis. use_gpu : [ bool ] Use GPU device when available. Preparation Configuration preparation_configuration : Parameters for data preparation. regression_target : [ str ] Target variable for regression. regression_target_label : [ str ] Label for the regression target. nparticles : [ int ] Number of selected final state particles. batch_size : [ int ] Size of the data batches. norm : [ bool ] Normalize samples, preserving only shape differences. duplicate : [ bool ] Duplicate training statistics for low input statistics. validation_plots : [ bool ] Produce input data validation plots. validation_plots_log : [ bool ] Use log scale for y-axis. Model and Training Hyperparameters transformer_classification_parameters : Hyperparameters for the transformer classification model. model_name : [ str ] Name of the model. nMHAlayers : [ int ] Number of multi-head attention layers. nheads : [ int ] Number of attention heads per multi-head attention layer. nDlayers : [ int ] Number of dense layers. vdropout : [ float ] Dropout factor. act_fn : [ str ] Activation function. nepochs : [ int ] Number of training epochs. learning_rate : [ float ] Learning rate. verbose : [ int ] Displayed information during training. embedding : [ bool ] Include an embedding layer. embedding_dim : [ int ] Dimension of the embedding layer. Validation Configuration validation_configuration : Configuration for the validation phase. luminosity_scaling : [ float ] Rescale samples to the given luminosity. save_predictions : [ bool ] Save predictions in HFD5 and ROOT file formats during validation process. save_onnx_model : [ bool ] Save best model into ONNX format. plot_model : [ bool ] Plot model architecture. plot_embedding : [ bool ] Plot embedding visualization. plot_confusion : [ bool ] Plot confusion matrix. plot_scores : [ bool ] Plot training performance scores (accuracy, precision, recall, F1, etc.). plot_discriminant : [ bool ] Plot LLR discriminant distributions. plot_proba : [ bool ] Plot output probabilities distributions. plot_roc : [ bool ] Plot ROC curves. plot_efficiency : [ bool ] Plot efficiency curves. plot_log_probabilities : [ bool ] Plot network output distributions with a log-scale y-axis. plot_log_discriminant : [ bool ] Plot network output distributions with a log-scale y-axis.","title":"Configuration"},{"location":"configuration/#configuration","text":"The following lines rovides a comprehensive guide to the configuration file in YAML format used for the analysis. The file includes specifications for training and validation samples, input variables, general settings, data preparation parameters, validation configurations, and model hyperparameters. Each section is explained in detail to assist users in understanding and customizing the analysis settings.","title":"Configuration"},{"location":"configuration/#training-and-validation-samples","text":"training_samples : List of training samples, each identified by a label. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ] Color for visualization. type : [ str ] Type of the sample (signal or background). validation_samples : List of validation samples, similar to training samples. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ]Color for visualization. type : [ str ] Type of the sample (signal or background).","title":"Training and Validation Samples"},{"location":"configuration/#input-variables","text":"input_variables : List of final state particle features used in the analysis. ghost_variables : List of ghost event features to be included in prediction file for validation.","title":"Input Variables"},{"location":"configuration/#general-configuration","text":"general_configuration : General settings for the analysis. output_directory : [ str ] Directory for storing output files. training_mode : [ str ] Accepted modes: \"classification\" or \"regression\". analysis_title : [ str ] Title for the analysis. use_gpu : [ bool ] Use GPU device when available.","title":"General Configuration"},{"location":"configuration/#preparation-configuration","text":"preparation_configuration : Parameters for data preparation. regression_target : [ str ] Target variable for regression. regression_target_label : [ str ] Label for the regression target. nparticles : [ int ] Number of selected final state particles. batch_size : [ int ] Size of the data batches. norm : [ bool ] Normalize samples, preserving only shape differences. duplicate : [ bool ] Duplicate training statistics for low input statistics. validation_plots : [ bool ] Produce input data validation plots. validation_plots_log : [ bool ] Use log scale for y-axis.","title":"Preparation Configuration"},{"location":"configuration/#model-and-training-hyperparameters","text":"transformer_classification_parameters : Hyperparameters for the transformer classification model. model_name : [ str ] Name of the model. nMHAlayers : [ int ] Number of multi-head attention layers. nheads : [ int ] Number of attention heads per multi-head attention layer. nDlayers : [ int ] Number of dense layers. vdropout : [ float ] Dropout factor. act_fn : [ str ] Activation function. nepochs : [ int ] Number of training epochs. learning_rate : [ float ] Learning rate. verbose : [ int ] Displayed information during training. embedding : [ bool ] Include an embedding layer. embedding_dim : [ int ] Dimension of the embedding layer.","title":"Model and Training Hyperparameters"},{"location":"configuration/#validation-configuration","text":"validation_configuration : Configuration for the validation phase. luminosity_scaling : [ float ] Rescale samples to the given luminosity. save_predictions : [ bool ] Save predictions in HFD5 and ROOT file formats during validation process. save_onnx_model : [ bool ] Save best model into ONNX format. plot_model : [ bool ] Plot model architecture. plot_embedding : [ bool ] Plot embedding visualization. plot_confusion : [ bool ] Plot confusion matrix. plot_scores : [ bool ] Plot training performance scores (accuracy, precision, recall, F1, etc.). plot_discriminant : [ bool ] Plot LLR discriminant distributions. plot_proba : [ bool ] Plot output probabilities distributions. plot_roc : [ bool ] Plot ROC curves. plot_efficiency : [ bool ] Plot efficiency curves. plot_log_probabilities : [ bool ] Plot network output distributions with a log-scale y-axis. plot_log_discriminant : [ bool ] Plot network output distributions with a log-scale y-axis.","title":"Validation Configuration"},{"location":"copyrights/","text":"Copyrights and Credits Final State Transformer is licensed under the MIT license. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files, to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","title":"Copyrights and Credits"},{"location":"copyrights/#copyrights-and-credits","text":"Final State Transformer is licensed under the MIT license. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files, to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","title":"Copyrights and Credits"},{"location":"input/","text":"Input Data Preparation The preparation script serves as a comprehensive tool for preparing training and validation datasets. Users can customize the data preparation process through the configuration file, specifying paths to data files, training modes (classification or regression), and other relevant parameters. The script then extracts particle datasets, applies normalization to ensure consistent weights, and prepares TensorFlow datasets for training. It offers options for reshuffling and batching, as well as the ability to artificially increase statistics by duplicating the dataset. The resulting datasets are saved in TensorFlow format, and the script optionally generates validation plots for feature engineering studies. With a user-friendly command-line interface, this script streamlines the intricate process of the data preparation for the training step. Example of input feature distributions: To optimize the utilization of Final State Transformer, input data should be provided in the HDF5 file format with a specific compound dataset structure. Input data are organized into structured formats for a particle_dataset and an event_dataset, for which each attribute features outlined in the used configuration file should be included. The HDF5 File Creation can be achieved by using tools or libraries such as h5py (Python) or HDF5 libraries in languages like C or Java. Find below an example of particle dataset structure. Example of particle dataset structure:","title":"Input Data Preparation"},{"location":"input/#input-data-preparation","text":"The preparation script serves as a comprehensive tool for preparing training and validation datasets. Users can customize the data preparation process through the configuration file, specifying paths to data files, training modes (classification or regression), and other relevant parameters. The script then extracts particle datasets, applies normalization to ensure consistent weights, and prepares TensorFlow datasets for training. It offers options for reshuffling and batching, as well as the ability to artificially increase statistics by duplicating the dataset. The resulting datasets are saved in TensorFlow format, and the script optionally generates validation plots for feature engineering studies. With a user-friendly command-line interface, this script streamlines the intricate process of the data preparation for the training step. Example of input feature distributions: To optimize the utilization of Final State Transformer, input data should be provided in the HDF5 file format with a specific compound dataset structure. Input data are organized into structured formats for a particle_dataset and an event_dataset, for which each attribute features outlined in the used configuration file should be included. The HDF5 File Creation can be achieved by using tools or libraries such as h5py (Python) or HDF5 libraries in languages like C or Java. Find below an example of particle dataset structure. Example of particle dataset structure:","title":"Input Data Preparation"},{"location":"model/","text":"Transformer Model An implementation of a modular Transformer encoder model is available in the model.py script, where the build_transformer function builds a multi-head attention-based classifier model using TensorFlow\u2019s Keras API. Illustration of a configurable model architecture with 1 embedding layer, 2 multi-head attention layers and 1 dense layer for a 4-classes classification problem: Input Layer The input layer sets the stage for passing input data into the subsequent layers of the Transformer model. A Keras input tensor is instantiated using tensorflow.keras.Input to create a placeholder tensor that is used as the input in the model. The input tensor is expetected to have a shape of (nparticles, nfeatures) , where nparticles represents the number of final state particles in the input sequence, and nfeatures represents the number of kinematic features for each particle. Embedding Layer An optional embedding layer can be instrumental for eleviating performance of the network. For instance it can be used for transforming categorical or discrete input features into dense, continuous vector representations that can be learned during training. For tasks that require a better data representation than the input one, such as unfolding the input space to reveal intricate information that the network can better utilize, the embedding layer also proves to be invaluable, enabling the model to uncover and effectively utilize nuanced patterns and relationships within the input data. By learning the embeddings from the input data, the model can adapt to the specific task at hand, ultimately enhancing its ability to extract meaningful features and make accurate predictions. A custom FloatEmbedding class initializes embedding weights in its constructor, which are randomly initialized and trainable, and then performs the embedding lookup operation in its call method. The embedding layer is optionally included based on the embedding parameter, allowing for the transformation of input. The embedding_dim parameter defines the dimension of the embedded space for which the data from input feature space is project into. During the training process, the model learns the optimal transformation matrix built upon the trainable parameters. Purely linear transformation is considered so far, no additional activation function is included. Multi-Head Attention Layer The attention mechanism in multi-head attention layers is a crucial component in Transformer models. It allows a model to weigh the importance of different final state particles in the input sequence when processing each event. This mechanism enables the model to capture long-range dependencies and relationships between the final state particles. In a multi-head attention layer, the attention mechanism is applied multiple times in parallel, each with its own set of learnable parameters. This results in multiple sets of attention weights being computed simultaneously, providing the model with multiple perspectives or \"heads\" to attend to different parts of the input sequence. In the context of multi-head attention mechanisms, the input sequence is transformed into the so-called query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices which play a crucial roles in aggregating information across the input sequence. The query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices are obtained by linear transformations of the input tensor \\(Q=XW_{Q}\\) , \\(K=KW_{K}\\) and \\(V=VW_{V}\\) , where \\(W_{Q}\\) , \\(W_{K}\\) and \\(W_{V}\\) are learnable weight matrices. These learnable parameters are optimized during the training process allowing the model to adapt and learn representations that are most relevant for the tast at hand. Attention scores are then computed between each pair of input sequence elements from the dot product of query and keyvectors and scaled by the square root of the dimensionality \\(d_{k}\\) . These attention scores represent the relevance or importance of other final state particles in the input sequence with respect to the current one. The attention scores are then scaled and passed through a softmax function to obtain attention weights. These weights determine how much each final state particle contributes to the representation of the current one. Finally, the weighted sum of the value vectors, where the weights are given by the attention weights, is computed. This results in a context vector for each final state particle, capturing information from other particles in the sequence based on their importance. \\(A(Q,K,V) = softmax\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\) In multi-head attention layers, this process is repeated across multiple heads, each with its own set of query, key, and value vectors in general. The output of each head is then concatenated and linearly transformed to produce the final output of the multi-head attention layer. In practice, multi-head attention layers are added to the model using the specialized Keras layers tensorflow.keras.layers.MultiHeadAttention . The number of attention heads to be uses in the mulit-head attention mechanism is specified with dedicated configurable parameter. The dimensionality of the key vector is set equal of the number of final state particles ( nparticles ). In the context of the so-called \"self-attention\" attention approach, the same input is used as the query, key and value. Flatten Layer The flatten layer essentially collapses all dimensions except for the batch dimension, simplifying the data structure for subsequent layers. In practical terms, after passing through the multi-head attention layers, the tensorflow.keras.layers.Flatten layer reshapes the output into a linear format compatible with densely connected layers that can perform classification tasks. Dense Layer After flattening, the output of the multi-head attention layers is passed trough a tensorflow.keras.layers.Dense layer with nparticles x nfeatures neurons and a configurable activation function. Additional hidden dense layers can be added, each with the same number of neurons and activation function. These layers learn complex patterns and represnetations in the data though learned weights and biases in order to increase the model's capabilities to capture remnant nonlinear relationships in the data After each dense layer, dropout regularization is applied to prevent overfitting and improve the generalization. The tensorflow.keras.layers.Dropout layer randomly set a fraction of input inuts to zero during the training, introducing noice and reducing reliance on specific inputs. Output Layer Finally, an output layer serves the crucial role of producing the final predictions of the model. In classification tasks, the output layer assigns probabilitiies to each class, indicating the likelihood of the input belonging to each class. Softmax activation function ensures that the predicted probabilities sum up to 1 across all classes, facilitating interpretation and decision-making. In regression tasks, the output layer directly predicts continuous values without any activation function. The model learns to map input sequences to continuous values directly, optimizing for regression objectives such as minimizing mean squared error. In practive, depending on the specified training_mode , the output layer is constructed differently. For classification tasks ( training_mode == \"classification\" ), A dense layer with nclass neurons is added, followed by a softmax activation function. For regression tasks ( training_mode == \"regression\" ), A dense layer with a single neuron is added, representing the regression output.","title":"Transformer Model"},{"location":"model/#transformer-model","text":"An implementation of a modular Transformer encoder model is available in the model.py script, where the build_transformer function builds a multi-head attention-based classifier model using TensorFlow\u2019s Keras API. Illustration of a configurable model architecture with 1 embedding layer, 2 multi-head attention layers and 1 dense layer for a 4-classes classification problem:","title":"Transformer Model"},{"location":"model/#input-layer","text":"The input layer sets the stage for passing input data into the subsequent layers of the Transformer model. A Keras input tensor is instantiated using tensorflow.keras.Input to create a placeholder tensor that is used as the input in the model. The input tensor is expetected to have a shape of (nparticles, nfeatures) , where nparticles represents the number of final state particles in the input sequence, and nfeatures represents the number of kinematic features for each particle.","title":"Input Layer"},{"location":"model/#embedding-layer","text":"An optional embedding layer can be instrumental for eleviating performance of the network. For instance it can be used for transforming categorical or discrete input features into dense, continuous vector representations that can be learned during training. For tasks that require a better data representation than the input one, such as unfolding the input space to reveal intricate information that the network can better utilize, the embedding layer also proves to be invaluable, enabling the model to uncover and effectively utilize nuanced patterns and relationships within the input data. By learning the embeddings from the input data, the model can adapt to the specific task at hand, ultimately enhancing its ability to extract meaningful features and make accurate predictions. A custom FloatEmbedding class initializes embedding weights in its constructor, which are randomly initialized and trainable, and then performs the embedding lookup operation in its call method. The embedding layer is optionally included based on the embedding parameter, allowing for the transformation of input. The embedding_dim parameter defines the dimension of the embedded space for which the data from input feature space is project into. During the training process, the model learns the optimal transformation matrix built upon the trainable parameters. Purely linear transformation is considered so far, no additional activation function is included.","title":"Embedding Layer"},{"location":"model/#multi-head-attention-layer","text":"The attention mechanism in multi-head attention layers is a crucial component in Transformer models. It allows a model to weigh the importance of different final state particles in the input sequence when processing each event. This mechanism enables the model to capture long-range dependencies and relationships between the final state particles. In a multi-head attention layer, the attention mechanism is applied multiple times in parallel, each with its own set of learnable parameters. This results in multiple sets of attention weights being computed simultaneously, providing the model with multiple perspectives or \"heads\" to attend to different parts of the input sequence. In the context of multi-head attention mechanisms, the input sequence is transformed into the so-called query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices which play a crucial roles in aggregating information across the input sequence. The query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices are obtained by linear transformations of the input tensor \\(Q=XW_{Q}\\) , \\(K=KW_{K}\\) and \\(V=VW_{V}\\) , where \\(W_{Q}\\) , \\(W_{K}\\) and \\(W_{V}\\) are learnable weight matrices. These learnable parameters are optimized during the training process allowing the model to adapt and learn representations that are most relevant for the tast at hand. Attention scores are then computed between each pair of input sequence elements from the dot product of query and keyvectors and scaled by the square root of the dimensionality \\(d_{k}\\) . These attention scores represent the relevance or importance of other final state particles in the input sequence with respect to the current one. The attention scores are then scaled and passed through a softmax function to obtain attention weights. These weights determine how much each final state particle contributes to the representation of the current one. Finally, the weighted sum of the value vectors, where the weights are given by the attention weights, is computed. This results in a context vector for each final state particle, capturing information from other particles in the sequence based on their importance. \\(A(Q,K,V) = softmax\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\) In multi-head attention layers, this process is repeated across multiple heads, each with its own set of query, key, and value vectors in general. The output of each head is then concatenated and linearly transformed to produce the final output of the multi-head attention layer. In practice, multi-head attention layers are added to the model using the specialized Keras layers tensorflow.keras.layers.MultiHeadAttention . The number of attention heads to be uses in the mulit-head attention mechanism is specified with dedicated configurable parameter. The dimensionality of the key vector is set equal of the number of final state particles ( nparticles ). In the context of the so-called \"self-attention\" attention approach, the same input is used as the query, key and value.","title":"Multi-Head Attention Layer"},{"location":"model/#flatten-layer","text":"The flatten layer essentially collapses all dimensions except for the batch dimension, simplifying the data structure for subsequent layers. In practical terms, after passing through the multi-head attention layers, the tensorflow.keras.layers.Flatten layer reshapes the output into a linear format compatible with densely connected layers that can perform classification tasks.","title":"Flatten Layer"},{"location":"model/#dense-layer","text":"After flattening, the output of the multi-head attention layers is passed trough a tensorflow.keras.layers.Dense layer with nparticles x nfeatures neurons and a configurable activation function. Additional hidden dense layers can be added, each with the same number of neurons and activation function. These layers learn complex patterns and represnetations in the data though learned weights and biases in order to increase the model's capabilities to capture remnant nonlinear relationships in the data After each dense layer, dropout regularization is applied to prevent overfitting and improve the generalization. The tensorflow.keras.layers.Dropout layer randomly set a fraction of input inuts to zero during the training, introducing noice and reducing reliance on specific inputs.","title":"Dense Layer"},{"location":"model/#output-layer","text":"Finally, an output layer serves the crucial role of producing the final predictions of the model. In classification tasks, the output layer assigns probabilitiies to each class, indicating the likelihood of the input belonging to each class. Softmax activation function ensures that the predicted probabilities sum up to 1 across all classes, facilitating interpretation and decision-making. In regression tasks, the output layer directly predicts continuous values without any activation function. The model learns to map input sequences to continuous values directly, optimizing for regression objectives such as minimizing mean squared error. In practive, depending on the specified training_mode , the output layer is constructed differently. For classification tasks ( training_mode == \"classification\" ), A dense layer with nclass neurons is added, followed by a softmax activation function. For regression tasks ( training_mode == \"regression\" ), A dense layer with a single neuron is added, representing the regression output.","title":"Output Layer"},{"location":"training/","text":"Training The FST\u2019s training procedure is a structured process focused on optimizing the configured Transformer model\u2019s ability to accurately predict outcomes based on the prepared input data. This procedure is adapted based on whether the task is classification or regression. General Process Training proceeds in epochs, where the entire dataset is passed through the model multiple times. During each epoch, the data is processed in batches, with the model making predictions for each batch and calculating the associated loss. Backpropagation is then used to compute the gradients of the loss function with respect to the model\u2019s parameters. These gradients are passed to the Adam optimizer, which updates the model\u2019s parameters accordingly. This iterative process allows the model to progressively improve its performance. To ensure robust performance and to prevent overfitting, the training process often includes regular evaluations on a validation set. This helps monitor the model\u2019s ability to generalize to unseen data. Additionally, checkpointing is implemented to save the model\u2019s state at various points during training, allowing for recovery if the process is interrupted. Early stopping can also be employed, halting the training if the model\u2019s performance on the validation set stops improving, thus avoiding unnecessary computations and potential overfitting. Classification and Regression Metrics In the current implementation, the Categorical Cross-Entropy Loss function is used as the primary metric for evaluating the classification model\u2019s predictions. This loss function is well-suited for tasks where the model outputs a probability distribution over different classes. It measures the performance of the classification model by quantifying the difference between the predicted probability distribution and the true distribution. Alongside the primary loss function, secondary metrics such as accuracy and AUC (Area Under the Curve). The accuracy measures the proportion of correctly predicted instances among the total instances, while the AUC metric evaluates the model\u2019s ability to distinguish between classes and is especially useful for understanding the performance across different threshold levels. AUC refers to the area under the ROC (Receiver Operating Characteristic) curve. The ROC curve plots the true positive rate against the false positive rate at various threshold settings. In case of regression tasks, the Mean Squared (MSE) Error is used as loss function. This is suitable for the models that predict continuous values. It measures the average squared difference between the predicted and actual values, providing a metric for how well the model\u2019s predictions match the true values. Alongside the primary loss function, secondary metrics such as accuracy and AUC (Area Under the Curve). Both classification or regression metrics are monitored and managed during the model training using specific Tensorflow callbacks. These callbacks provide functionality for early stopping, model checkpointing, and logging metrics for visualization. The Fig. 4 gives an illustration of loss, accuracy and AUC curves for training and validation of classification models, which are updated after each epoch during the training. Example of usual training curves for monitoring classification tasks: Optimizer To optimize the model\u2019s parameters, the Adam optimizer is employed. Adam is a popular choice in deep learning due to its adaptive learning rate, which adjusts the learning rate for each parameter individually based on estimates of first and second moments of the gradients. This results in more efficient and faster convergence, especially in complex models like transformers. The Adam optimizer updates the model\u2019s weights based on the gradients computed from the loss, gradually refining the model\u2019s predictions over successive iterations.","title":"Training"},{"location":"training/#training","text":"The FST\u2019s training procedure is a structured process focused on optimizing the configured Transformer model\u2019s ability to accurately predict outcomes based on the prepared input data. This procedure is adapted based on whether the task is classification or regression.","title":"Training"},{"location":"training/#general-process","text":"Training proceeds in epochs, where the entire dataset is passed through the model multiple times. During each epoch, the data is processed in batches, with the model making predictions for each batch and calculating the associated loss. Backpropagation is then used to compute the gradients of the loss function with respect to the model\u2019s parameters. These gradients are passed to the Adam optimizer, which updates the model\u2019s parameters accordingly. This iterative process allows the model to progressively improve its performance. To ensure robust performance and to prevent overfitting, the training process often includes regular evaluations on a validation set. This helps monitor the model\u2019s ability to generalize to unseen data. Additionally, checkpointing is implemented to save the model\u2019s state at various points during training, allowing for recovery if the process is interrupted. Early stopping can also be employed, halting the training if the model\u2019s performance on the validation set stops improving, thus avoiding unnecessary computations and potential overfitting.","title":"General Process"},{"location":"training/#classification-and-regression-metrics","text":"In the current implementation, the Categorical Cross-Entropy Loss function is used as the primary metric for evaluating the classification model\u2019s predictions. This loss function is well-suited for tasks where the model outputs a probability distribution over different classes. It measures the performance of the classification model by quantifying the difference between the predicted probability distribution and the true distribution. Alongside the primary loss function, secondary metrics such as accuracy and AUC (Area Under the Curve). The accuracy measures the proportion of correctly predicted instances among the total instances, while the AUC metric evaluates the model\u2019s ability to distinguish between classes and is especially useful for understanding the performance across different threshold levels. AUC refers to the area under the ROC (Receiver Operating Characteristic) curve. The ROC curve plots the true positive rate against the false positive rate at various threshold settings. In case of regression tasks, the Mean Squared (MSE) Error is used as loss function. This is suitable for the models that predict continuous values. It measures the average squared difference between the predicted and actual values, providing a metric for how well the model\u2019s predictions match the true values. Alongside the primary loss function, secondary metrics such as accuracy and AUC (Area Under the Curve). Both classification or regression metrics are monitored and managed during the model training using specific Tensorflow callbacks. These callbacks provide functionality for early stopping, model checkpointing, and logging metrics for visualization. The Fig. 4 gives an illustration of loss, accuracy and AUC curves for training and validation of classification models, which are updated after each epoch during the training. Example of usual training curves for monitoring classification tasks:","title":"Classification and Regression Metrics"},{"location":"training/#optimizer","text":"To optimize the model\u2019s parameters, the Adam optimizer is employed. Adam is a popular choice in deep learning due to its adaptive learning rate, which adjusts the learning rate for each parameter individually based on estimates of first and second moments of the gradients. This results in more efficient and faster convergence, especially in complex models like transformers. The Adam optimizer updates the model\u2019s weights based on the gradients computed from the loss, gradually refining the model\u2019s predictions over successive iterations.","title":"Optimizer"},{"location":"troubleshooting/","text":"Troubleshooting By following these troubleshooting steps, you can effectively address and resolve common issues encountered while using the Final State Transformer toolkit. Common Issues and Solutions Installation Problems Unable to install dependencies : Ensure you have the correct version of Python and pip installed. Run python --version and pip --version to verify. If the versions are incorrect, download and install the appropriate versions from the official Python website. Errors during pip install -r requirements.txt : Check the error message for missing packages or compatibility issues. Ensure your system has the necessary compilers and libraries. Problems with virtual environment activation. : Ensure the virtual environment is correctly set up. On Unix-based systems, use source venv/bin/activate . If activation fails, check the path and permissions. Data Loading Errors Incorrect data format : Ensure your data is in the expected format, such as HDF5. Refer to the data preparation section of the documentation for specific formatting requirements. Verify the integrity of the data files and ensure there are no missing or corrupted entries. FileNotFoundError or path issues : Double-check the paths specified in your configuration files. Ensure the paths are absolute or correctly relative to the working directory. Confirm that the files exist at the specified locations. Training Failures Model training crashes or does not start : Review the configuration files for any incorrect or missing parameters. Ensure that the dataset paths, model parameters, and training hyperparameters are correctly specified. Check the logs for specific error messages and troubleshoot accordingly. CUDA-related errors (if using GPU) : Verify that CUDA is properly installed and that your GPU drivers are up to date. Use nvidia-smi to check GPU status. Ensure that the TensorFlow or PyTorch version you are using is compatible with your CUDA version. Memory errors or out-of-memory issues : Reduce batch size or use a smaller model architecture to fit within your system's memory limits. If using a GPU, ensure that other processes are not occupying significant GPU memory. Evaluation Discrepancies Unexpectedly low performance metrics : Verify that the model checkpoint being loaded for evaluation is the correct one. Ensure that the evaluation dataset is correctly formatted and preprocessed. Cross-check the evaluation script parameters and configuration. Inconsistent results between training and evaluation : Check for data leakage, where information from the training set inadvertently ends up in the evaluation set. Ensure that data augmentation and preprocessing steps are applied consistently during training and evaluation. Debugging Tips Log Files : Thoroughly review log files generated during training and evaluation for detailed error messages and warnings. Logs can provide insights into the stages where issues occur and help identify the root cause. Verbose Mode : Run scripts in verbose mode to get more detailed output. This can be done by adding verbosity flags (e.g., --verbose or -v ) to your command, which can help in pinpointing where the process is failing. Dependency Checks : Regularly check for updates to dependencies and ensure compatibility. Using a package manager like pip-tools can help manage and update dependencies systematically. Sanity Checks : Perform sanity checks on your data and model by running a few quick epochs with a small subset of the data. This can help identify obvious issues before committing to longer training sessions. Specific Error Messages and Their Resolutions ModuleNotFoundError : Ensure that all necessary packages are installed. If you are working in a virtual environment, ensure it is activated. You may also need to update your PYTHONPATH to include the directory where the module resides. ValueError: Shapes (X, Y) and (A, B) are incompatible : This error typically indicates a mismatch in expected input shapes. Verify the dimensions of your input data and ensure they match the model's expected input. Adjust preprocessing steps if necessary. TypeError: object of type 'NoneType' has no len() : This error suggests that a variable expected to be a list or similar iterable is None . Trace the variable through your code to ensure it is being properly initialized and populated.","title":"Troubleshouting"},{"location":"troubleshooting/#troubleshooting","text":"By following these troubleshooting steps, you can effectively address and resolve common issues encountered while using the Final State Transformer toolkit.","title":"Troubleshooting"},{"location":"troubleshooting/#common-issues-and-solutions","text":"","title":"Common Issues and Solutions"},{"location":"troubleshooting/#installation-problems","text":"Unable to install dependencies : Ensure you have the correct version of Python and pip installed. Run python --version and pip --version to verify. If the versions are incorrect, download and install the appropriate versions from the official Python website. Errors during pip install -r requirements.txt : Check the error message for missing packages or compatibility issues. Ensure your system has the necessary compilers and libraries. Problems with virtual environment activation. : Ensure the virtual environment is correctly set up. On Unix-based systems, use source venv/bin/activate . If activation fails, check the path and permissions.","title":"Installation Problems"},{"location":"troubleshooting/#data-loading-errors","text":"Incorrect data format : Ensure your data is in the expected format, such as HDF5. Refer to the data preparation section of the documentation for specific formatting requirements. Verify the integrity of the data files and ensure there are no missing or corrupted entries. FileNotFoundError or path issues : Double-check the paths specified in your configuration files. Ensure the paths are absolute or correctly relative to the working directory. Confirm that the files exist at the specified locations.","title":"Data Loading Errors"},{"location":"troubleshooting/#training-failures","text":"Model training crashes or does not start : Review the configuration files for any incorrect or missing parameters. Ensure that the dataset paths, model parameters, and training hyperparameters are correctly specified. Check the logs for specific error messages and troubleshoot accordingly. CUDA-related errors (if using GPU) : Verify that CUDA is properly installed and that your GPU drivers are up to date. Use nvidia-smi to check GPU status. Ensure that the TensorFlow or PyTorch version you are using is compatible with your CUDA version. Memory errors or out-of-memory issues : Reduce batch size or use a smaller model architecture to fit within your system's memory limits. If using a GPU, ensure that other processes are not occupying significant GPU memory.","title":"Training Failures"},{"location":"troubleshooting/#evaluation-discrepancies","text":"Unexpectedly low performance metrics : Verify that the model checkpoint being loaded for evaluation is the correct one. Ensure that the evaluation dataset is correctly formatted and preprocessed. Cross-check the evaluation script parameters and configuration. Inconsistent results between training and evaluation : Check for data leakage, where information from the training set inadvertently ends up in the evaluation set. Ensure that data augmentation and preprocessing steps are applied consistently during training and evaluation.","title":"Evaluation Discrepancies"},{"location":"troubleshooting/#debugging-tips","text":"Log Files : Thoroughly review log files generated during training and evaluation for detailed error messages and warnings. Logs can provide insights into the stages where issues occur and help identify the root cause. Verbose Mode : Run scripts in verbose mode to get more detailed output. This can be done by adding verbosity flags (e.g., --verbose or -v ) to your command, which can help in pinpointing where the process is failing. Dependency Checks : Regularly check for updates to dependencies and ensure compatibility. Using a package manager like pip-tools can help manage and update dependencies systematically. Sanity Checks : Perform sanity checks on your data and model by running a few quick epochs with a small subset of the data. This can help identify obvious issues before committing to longer training sessions.","title":"Debugging Tips"},{"location":"troubleshooting/#specific-error-messages-and-their-resolutions","text":"ModuleNotFoundError : Ensure that all necessary packages are installed. If you are working in a virtual environment, ensure it is activated. You may also need to update your PYTHONPATH to include the directory where the module resides. ValueError: Shapes (X, Y) and (A, B) are incompatible : This error typically indicates a mismatch in expected input shapes. Verify the dimensions of your input data and ensure they match the model's expected input. Adjust preprocessing steps if necessary. TypeError: object of type 'NoneType' has no len() : This error suggests that a variable expected to be a list or similar iterable is None . Trace the variable through your code to ensure it is being properly initialized and populated.","title":"Specific Error Messages and Their Resolutions"},{"location":"validation/","text":"Performance Assessment The validation scripts load the trained model, performs predictions on validation datasets, and conducts various validation tasks based on user-defined options. These tasks include saving predictions, plotting model architecture, computing and visualizing confusion matrices, generating ROC curves, plotting efficiency curves, and displaying probability distributions. The script is adaptable to both classification and regression scenarios, offering different visualization and evaluation methods based on the selected training mode. The command-line interface allows users to specify the configuration file path when executing the script, contributing to ease of use and configurability. Overall, the script provides a comprehensive set of validation tools for assessing the performance of a Transformer model on new data. Confusion Matrix A confusion matrix is a key performance measurement tool which allows you to visualise, in a tabular format, the performance of a classifier by comparing predicted classes against true classes. It helps identifying which classes are being confused with each other, giving insights into areas where your model may need improvements. Beyond the various metrics which can be calculated, such as accuracy, precision, recall, specificity or F1 score, the examination of its cells can provide insights into the types of errors made by the model. For example, a high number of false positives may indicate that the model is overly sensitive and is incorrectly predicting positive outcomes too often. On the other hand, a high number of false negatives may suggest that the model is missing important patterns or features in the data. Training Performance Scores Several scores are computed and plotted, in order to evaluate the performance of classification models. Accuracy : Accuracy is a measure of the overall correctness of a model. It is calculated as the ratio of correctly predicted instances to the total instances in the dataset: \\(A = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}.\\) Precision : Precision is the measure of the correctness of positive predictions made by the model. It is the ratio of true positive predictions to the total predicted positive instances: \\(P = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}.\\) Recall (Sensitivity) : Recall, also known as Sensitivity, is the measure of the model's ability to correctly identify positive instances out of all actual positive instances. It is calculated as the ratio of true positive predictions to the total actual positive instances: \\(R = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}.\\) F1 Score : The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall: \\(F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\\) Misclassification Difficulty : Misclassification difficulty refers to the complexity or degree of difficulty in accurately classifying instances in a dataset. It is subjective and can vary depending on factors such as class imbalance, overlap between classes, and the quality of features used for classification. Higher misclassification difficulty implies that it is more challenging for a model to correctly classify instances. Output Class Probabilities The output probability class of classification models refers to the probability assigned by the model to each class for a given input instance. In classification tasks, especially in scenarios with more than two classes (multi-class classification), the model predicts the probability distribution over all possible classes. Each class is assigned a probability score indicating the likelihood of the input instance belonging to that class. The class with the highest probability is typically chosen as the predicted class for the input instance. However, you can adjust the threshold depending on your requirements. Output probability classes are useful not only for making predictions but also for assessing the model's confidence in its predictions. Higher probabilities generally indicate higher confidence in the prediction, while lower probabilities suggest more uncertainty. LLR Discriminant The LLR (Log-Likelihood Ratio) ratio discriminant is a statistical measure used in binary classification tasks, particularly in signal detection scenarios, to distinguish between a signal and background noise. It is based on comparing the likelihoods of an observation under the signal hypothesis versus the background hypothesis. The LLR ratio discriminant represents the logarithm of the likelihood ratio of the signal probability to the sum of background probabilities for a given input instance. In signal detection problems, the model assigns probabilities to both the signal and background classes. The LLR ratio discriminant is calculated using these probabilities and is given by: \\(D = \\ln (\\frac{\\text{Signal Probability}}{\\text{Sum of Background Probabilities}})\\) A positive LLR ratio discriminant suggests that the observation is more likely to belong to the signal class, while a negative value indicates a higher likelihood of belonging to the background class. The magnitude of the LLR ratio discriminant indicates the strength of evidence for one class over the other. For example, if the LLR ratio discriminant for an observation is 2.5, it means that the observation is 10^(2.5) times more likely to belong to the signal class compared to the background class. The LLR ratio discriminant is particularly useful in scenarios where there is a need to distinguish a weak signal from background noise with high confidence. ROC Curves Receiver Operating Characteristic (ROC) curves are indispensable tools for assessing the performance of classification models, especially in scenarios where background rejection is a crucial consideration relative to signal efficiency. On the deficated validation plots, the top-right corner of the ROC space represents the optimal performance, where the model achieves high signal efficiency while maintaining high background rejection. The Area Under the ROC Curve (AUC-ROC) quantifies the overall performance of the model, with higher values indicating superior discrimination between signal and background. ROC analysis facilitates the comparison of different models' performances, helping in the selection and optimization of the model.","title":"Performance Assessment"},{"location":"validation/#performance-assessment","text":"The validation scripts load the trained model, performs predictions on validation datasets, and conducts various validation tasks based on user-defined options. These tasks include saving predictions, plotting model architecture, computing and visualizing confusion matrices, generating ROC curves, plotting efficiency curves, and displaying probability distributions. The script is adaptable to both classification and regression scenarios, offering different visualization and evaluation methods based on the selected training mode. The command-line interface allows users to specify the configuration file path when executing the script, contributing to ease of use and configurability. Overall, the script provides a comprehensive set of validation tools for assessing the performance of a Transformer model on new data.","title":"Performance Assessment"},{"location":"validation/#confusion-matrix","text":"A confusion matrix is a key performance measurement tool which allows you to visualise, in a tabular format, the performance of a classifier by comparing predicted classes against true classes. It helps identifying which classes are being confused with each other, giving insights into areas where your model may need improvements. Beyond the various metrics which can be calculated, such as accuracy, precision, recall, specificity or F1 score, the examination of its cells can provide insights into the types of errors made by the model. For example, a high number of false positives may indicate that the model is overly sensitive and is incorrectly predicting positive outcomes too often. On the other hand, a high number of false negatives may suggest that the model is missing important patterns or features in the data.","title":"Confusion Matrix"},{"location":"validation/#training-performance-scores","text":"Several scores are computed and plotted, in order to evaluate the performance of classification models. Accuracy : Accuracy is a measure of the overall correctness of a model. It is calculated as the ratio of correctly predicted instances to the total instances in the dataset: \\(A = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}.\\) Precision : Precision is the measure of the correctness of positive predictions made by the model. It is the ratio of true positive predictions to the total predicted positive instances: \\(P = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}.\\) Recall (Sensitivity) : Recall, also known as Sensitivity, is the measure of the model's ability to correctly identify positive instances out of all actual positive instances. It is calculated as the ratio of true positive predictions to the total actual positive instances: \\(R = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}.\\) F1 Score : The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall: \\(F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\\) Misclassification Difficulty : Misclassification difficulty refers to the complexity or degree of difficulty in accurately classifying instances in a dataset. It is subjective and can vary depending on factors such as class imbalance, overlap between classes, and the quality of features used for classification. Higher misclassification difficulty implies that it is more challenging for a model to correctly classify instances.","title":"Training Performance Scores"},{"location":"validation/#output-class-probabilities","text":"The output probability class of classification models refers to the probability assigned by the model to each class for a given input instance. In classification tasks, especially in scenarios with more than two classes (multi-class classification), the model predicts the probability distribution over all possible classes. Each class is assigned a probability score indicating the likelihood of the input instance belonging to that class. The class with the highest probability is typically chosen as the predicted class for the input instance. However, you can adjust the threshold depending on your requirements. Output probability classes are useful not only for making predictions but also for assessing the model's confidence in its predictions. Higher probabilities generally indicate higher confidence in the prediction, while lower probabilities suggest more uncertainty.","title":"Output Class Probabilities"},{"location":"validation/#llr-discriminant","text":"The LLR (Log-Likelihood Ratio) ratio discriminant is a statistical measure used in binary classification tasks, particularly in signal detection scenarios, to distinguish between a signal and background noise. It is based on comparing the likelihoods of an observation under the signal hypothesis versus the background hypothesis. The LLR ratio discriminant represents the logarithm of the likelihood ratio of the signal probability to the sum of background probabilities for a given input instance. In signal detection problems, the model assigns probabilities to both the signal and background classes. The LLR ratio discriminant is calculated using these probabilities and is given by: \\(D = \\ln (\\frac{\\text{Signal Probability}}{\\text{Sum of Background Probabilities}})\\) A positive LLR ratio discriminant suggests that the observation is more likely to belong to the signal class, while a negative value indicates a higher likelihood of belonging to the background class. The magnitude of the LLR ratio discriminant indicates the strength of evidence for one class over the other. For example, if the LLR ratio discriminant for an observation is 2.5, it means that the observation is 10^(2.5) times more likely to belong to the signal class compared to the background class. The LLR ratio discriminant is particularly useful in scenarios where there is a need to distinguish a weak signal from background noise with high confidence.","title":"LLR Discriminant"},{"location":"validation/#roc-curves","text":"Receiver Operating Characteristic (ROC) curves are indispensable tools for assessing the performance of classification models, especially in scenarios where background rejection is a crucial consideration relative to signal efficiency. On the deficated validation plots, the top-right corner of the ROC space represents the optimal performance, where the model achieves high signal efficiency while maintaining high background rejection. The Area Under the ROC Curve (AUC-ROC) quantifies the overall performance of the model, with higher values indicating superior discrimination between signal and background. ROC analysis facilitates the comparison of different models' performances, helping in the selection and optimization of the model.","title":"ROC Curves"}]}