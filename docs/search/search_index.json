{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FINAL STATE TRANSFORMER Machine Learning development toolkit, built upon Transformer encoder network architectures and specifically tailored for the realm of high-energy physics and particle event analysis. This toolkit is designed to excel in the tasks of classification and regression on sequences of particle event final state objects. It capitalizes on the inherent ability of these Transformer architectures to capture intricate patterns and dependencies within sequential data. This makes it particularly well-suited for unraveling the complexities present in the final states of particle events, where the subtle interplay of particles usually demands a sophisticated understanding. The toolkit's adaptability extends beyond its architectural foundation. It incorporates advanced techniques for feature engineering, hyperparameter tuning, and model evaluation, ensuring that users can tailor the toolkit to the unique characteristics of their datasets and research objectives. Designed with a user-friendly interface, the toolkit facilitates seamless integration into research workflows, enabling scientists and researchers to harness the power of state-of-the-art machine learning without compromising on accessibility. Its open architecture encourages collaboration and allows for easy expansion to incorporate future advancements in the field of particle physics. Powered by:","title":"Home"},{"location":"#final-state-transformer","text":"Machine Learning development toolkit, built upon Transformer encoder network architectures and specifically tailored for the realm of high-energy physics and particle event analysis. This toolkit is designed to excel in the tasks of classification and regression on sequences of particle event final state objects. It capitalizes on the inherent ability of these Transformer architectures to capture intricate patterns and dependencies within sequential data. This makes it particularly well-suited for unraveling the complexities present in the final states of particle events, where the subtle interplay of particles usually demands a sophisticated understanding. The toolkit's adaptability extends beyond its architectural foundation. It incorporates advanced techniques for feature engineering, hyperparameter tuning, and model evaluation, ensuring that users can tailor the toolkit to the unique characteristics of their datasets and research objectives. Designed with a user-friendly interface, the toolkit facilitates seamless integration into research workflows, enabling scientists and researchers to harness the power of state-of-the-art machine learning without compromising on accessibility. Its open architecture encourages collaboration and allows for easy expansion to incorporate future advancements in the field of particle physics. Powered by:","title":"FINAL STATE TRANSFORMER"},{"location":"code/","text":"The Code Get the code Start by cloning the final-state-transformer repository. If you benefit from an ssh key for GitLab, you can proceed with: git clone git@github.com:dev-geof/final-state-transformer.git cd final-state-transformer Installation First ensure that you have Python 3.9.13 or higher version installed on your system. You can check your Python version by running the following command in your terminal: python --version If Python is not installed or is an older version, please download and install the latest version from the official Python website. You can install final-state-transformer within a virtual environement. The venv environement is a lightweight solution for creating virtual environement in python. Docker images are not yet available. To create a new virtual environement: python3 -m venv env source env/bin/activate To install final-state-transformer and its dependencies you can use the following command: python -m pip install -e . -r requirements.txt","title":"Code"},{"location":"code/#the-code","text":"","title":"The Code"},{"location":"code/#get-the-code","text":"Start by cloning the final-state-transformer repository. If you benefit from an ssh key for GitLab, you can proceed with: git clone git@github.com:dev-geof/final-state-transformer.git cd final-state-transformer","title":"Get the code"},{"location":"code/#installation","text":"First ensure that you have Python 3.9.13 or higher version installed on your system. You can check your Python version by running the following command in your terminal: python --version If Python is not installed or is an older version, please download and install the latest version from the official Python website. You can install final-state-transformer within a virtual environement. The venv environement is a lightweight solution for creating virtual environement in python. Docker images are not yet available. To create a new virtual environement: python3 -m venv env source env/bin/activate To install final-state-transformer and its dependencies you can use the following command: python -m pip install -e . -r requirements.txt","title":"Installation"},{"location":"configuration/","text":"Configuration File The following lines rovides a comprehensive guide to the configuration file in YAML format used for the analysis. The file includes specifications for training and validation samples, input variables, general settings, data preparation parameters, validation configurations, and model hyperparameters. Each section is explained in detail to assist users in understanding and customizing the analysis settings. Training and Validation Samples training_samples : List of training samples, each identified by a label. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ] Color for visualization. type : [ str ] Type of the sample (signal or background). validation_samples : List of validation samples, similar to training samples. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ]Color for visualization. type : [ str ] Type of the sample (signal or background). Input Variables input_variables : List of final state particle features used in the analysis. ghost_variables : List of ghost event features to be included in prediction file for validation. General Configuration general_configuration : General settings for the analysis. output_directory : [ str ] Directory for storing output files. training_mode : [ str ] Accepted modes: \"classification\" or \"regression\". analysis_title : [ str ] Title for the analysis. use_gpu : [ bool ] Use GPU device when available. Preparation Configuration preparation_configuration : Parameters for data preparation. regression_target : [ str ] Target variable for regression. regression_target_label : [ str ] Label for the regression target. nparticles : [ int ] Number of selected final state particles. batch_size : [ int ] Size of the data batches. norm : [ bool ] Normalize samples, preserving only shape differences. duplicate : [ bool ] Duplicate training statistics for low input statistics. validation_plots : [ bool ] Produce input data validation plots. validation_plots_log : [ bool ] Use log scale for y-axis. Model and Training Hyperparameters transformer_classification_parameters : Hyperparameters for the transformer classification model. model_name : [ str ] Name of the model. nMHAlayers : [ int ] Number of multi-head attention layers. nheads : [ int ] Number of attention heads per multi-head attention layer. nDlayers : [ int ] Number of dense layers. vdropout : [ float ] Dropout factor. act_fn : [ str ] Activation function. nepochs : [ int ] Number of training epochs. learning_rate : [ float ] Learning rate. verbose : [ int ] Displayed information during training. embedding : [ bool ] Include an embedding layer for categorical input variables (currently not operational). embedding_dim : [ int ] Dimension of the embedding layer (currently not operational). Validation Configuration validation_configuration : Configuration for the validation phase. luminosity_scaling : [ float ] Rescale samples to the given luminosity. save_predictions : [ bool ] Save predictions in HFD5 and ROOT file formats during validation process. save_onnx_model : [ bool ] Save best model into ONNX format. plot_model : [ bool ] Plot model architecture. plot_confusion : [ bool ] Plot confusion matrix. plot_scores : [ bool ] Plot training performance scores (accuracy, precision, recall, F1, etc.). plot_discriminant : [ bool ] Plot LLR discriminant distributions. plot_proba : [ bool ] Plot output probabilities distributions. plot_roc : [ bool ] Plot ROC curves. plot_efficiency : [ bool ] Plot efficiency curves. plot_log_probabilities : [ bool ] Plot network output distributions with a log-scale y-axis. plot_log_discriminant : [ bool ] Plot network output distributions with a log-scale y-axis.","title":"Configuration File"},{"location":"configuration/#configuration-file","text":"The following lines rovides a comprehensive guide to the configuration file in YAML format used for the analysis. The file includes specifications for training and validation samples, input variables, general settings, data preparation parameters, validation configurations, and model hyperparameters. Each section is explained in detail to assist users in understanding and customizing the analysis settings.","title":"Configuration File"},{"location":"configuration/#training-and-validation-samples","text":"training_samples : List of training samples, each identified by a label. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ] Color for visualization. type : [ str ] Type of the sample (signal or background). validation_samples : List of validation samples, similar to training samples. path : [ str ] Path to the sample data file. Several files can be included by separating them with \":\" symbol. event_dataset : [ str ] Name of the event dataset. particle_dataset : [ str ] Name of the particle dataset. weights : [ str ] Name of the MC weight variable. nevents : [ int ] Number of events to use (same for each file included, to be update). cross_section : [ float ] Process cross section. branching_ratio : [ float ] Decay branching ratio. acceptance_factor : [ float ] Additional acceptance correction factor. legend : [ str ] Legend label for the sample. colour : [ str ]Color for visualization. type : [ str ] Type of the sample (signal or background).","title":"Training and Validation Samples"},{"location":"configuration/#input-variables","text":"input_variables : List of final state particle features used in the analysis. ghost_variables : List of ghost event features to be included in prediction file for validation.","title":"Input Variables"},{"location":"configuration/#general-configuration","text":"general_configuration : General settings for the analysis. output_directory : [ str ] Directory for storing output files. training_mode : [ str ] Accepted modes: \"classification\" or \"regression\". analysis_title : [ str ] Title for the analysis. use_gpu : [ bool ] Use GPU device when available.","title":"General Configuration"},{"location":"configuration/#preparation-configuration","text":"preparation_configuration : Parameters for data preparation. regression_target : [ str ] Target variable for regression. regression_target_label : [ str ] Label for the regression target. nparticles : [ int ] Number of selected final state particles. batch_size : [ int ] Size of the data batches. norm : [ bool ] Normalize samples, preserving only shape differences. duplicate : [ bool ] Duplicate training statistics for low input statistics. validation_plots : [ bool ] Produce input data validation plots. validation_plots_log : [ bool ] Use log scale for y-axis.","title":"Preparation Configuration"},{"location":"configuration/#model-and-training-hyperparameters","text":"transformer_classification_parameters : Hyperparameters for the transformer classification model. model_name : [ str ] Name of the model. nMHAlayers : [ int ] Number of multi-head attention layers. nheads : [ int ] Number of attention heads per multi-head attention layer. nDlayers : [ int ] Number of dense layers. vdropout : [ float ] Dropout factor. act_fn : [ str ] Activation function. nepochs : [ int ] Number of training epochs. learning_rate : [ float ] Learning rate. verbose : [ int ] Displayed information during training. embedding : [ bool ] Include an embedding layer for categorical input variables (currently not operational). embedding_dim : [ int ] Dimension of the embedding layer (currently not operational).","title":"Model and Training Hyperparameters"},{"location":"configuration/#validation-configuration","text":"validation_configuration : Configuration for the validation phase. luminosity_scaling : [ float ] Rescale samples to the given luminosity. save_predictions : [ bool ] Save predictions in HFD5 and ROOT file formats during validation process. save_onnx_model : [ bool ] Save best model into ONNX format. plot_model : [ bool ] Plot model architecture. plot_confusion : [ bool ] Plot confusion matrix. plot_scores : [ bool ] Plot training performance scores (accuracy, precision, recall, F1, etc.). plot_discriminant : [ bool ] Plot LLR discriminant distributions. plot_proba : [ bool ] Plot output probabilities distributions. plot_roc : [ bool ] Plot ROC curves. plot_efficiency : [ bool ] Plot efficiency curves. plot_log_probabilities : [ bool ] Plot network output distributions with a log-scale y-axis. plot_log_discriminant : [ bool ] Plot network output distributions with a log-scale y-axis.","title":"Validation Configuration"},{"location":"input/","text":"Input Data Format To optimize the utilization of Final State Transformer, input data should be provided in the HDF5 file format with a specific compound dataset structure. Input data are organized into structured formats for a particle_dataset and an event_dataset , for which each attribute features outlined in the used configuration file should be included. The HDF5 File Creation can be achieved by using tools or libraries such as h5py (Python) or HDF5 libraries in languages like C or Java. Example of particle dataset structure:","title":"Input Data Format"},{"location":"input/#input-data-format","text":"To optimize the utilization of Final State Transformer, input data should be provided in the HDF5 file format with a specific compound dataset structure. Input data are organized into structured formats for a particle_dataset and an event_dataset , for which each attribute features outlined in the used configuration file should be included. The HDF5 File Creation can be achieved by using tools or libraries such as h5py (Python) or HDF5 libraries in languages like C or Java. Example of particle dataset structure:","title":"Input Data Format"},{"location":"model/","text":"Transformer Model An implementation of a modular Transformer encoder model is available in the model.py script, where the build_transformer function builds a multi-head attention-based classifier model using TensorFlow's Keras API. The main configurable hyper-parameters are detailed above in the configuration file section . Illustration of a configurable model architecture with 2 multi-head attention layers and 3 dense layers for a 4-classes classification problem: Input Layer The input layer sets the stage for passing input data into the subsequent layers of the Transformer model. A Keras input tensor is instantiated using tensorflow.keras.Input to create a placeholder tensor that is used as the input in the model. The input tensor is expetected to have a shape of (nparticles, nfeatures) , where nparticles represents the number of final state particles in the input sequence, and nfeatures represents the number of kinematic features for each particle. Multi-Head Attention Layer The attention mechanism in multi-head attention layers is a crucial component in Transformer models. It allows a model to weigh the importance of different final state particles in the input sequence when processing each event. This mechanism enables the model to capture long-range dependencies and relationships between the final state particles. In a multi-head attention layer, the attention mechanism is applied multiple times in parallel, each with its own set of learnable parameters. This results in multiple sets of attention weights being computed simultaneously, providing the model with multiple perspectives or \"heads\" to attend to different parts of the input sequence. In the context of multi-head attention mechanisms, the input sequence is transformed into the so-called query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices which play a crucial roles in aggregating information across the input sequence. The query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices are obtained by linear transformations of the input tensor \\(Q=XW_{Q}\\) , \\(K=KW_{K}\\) and \\(V=VW_{V}\\) , where \\(W_{Q}\\) , \\(W_{K}\\) and \\(W_{V}\\) are learnable weight matrices. These learnable parameters are optimized during the training process allowing the model to adapt and learn representations that are most relevant for the tast at hand. Attention scores are then computed between each pair of input sequence elements from the dot product of query and keyvectors and scaled by the square root of the dimensionality \\(d_{k}\\) . These attention scores represent the relevance or importance of other final state particles in the input sequence with respect to the current one. The attention scores are then scaled and passed through a softmax function to obtain attention weights. These weights determine how much each final state particle contributes to the representation of the current one. Finally, the weighted sum of the value vectors, where the weights are given by the attention weights, is computed. This results in a context vector for each final state particle, capturing information from other particles in the sequence based on their importance. \\(A(Q,K,V) = softmax\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\) In multi-head attention layers, this process is repeated across multiple heads, each with its own set of query, key, and value vectors in general. The output of each head is then concatenated and linearly transformed to produce the final output of the multi-head attention layer. In practice, multi-head attention layers are added to the model using the specialized Keras layers tensorflow.keras.layers.MultiHeadAttention . The number of attention heads to be uses in the mulit-head attention mechanism is specified with dedicated configurable parameter. The dimensionality of the key vector is set equal of the number of final state particles ( nparticles ). In the context of the so-called \"self-attention\" attention approach, the same input is used as the query, key and value. Flatten Layer The flatten layer essentially collapses all dimensions except for the batch dimension, simplifying the data structure for subsequent layers. In practical terms, after passing through the multi-head attention layers, the tensorflow.keras.layers.Flatten layer reshapes the output into a linear format compatible with densely connected layers that can perform classification tasks. Dense Layer After flattening, the output of the multi-head attention layers is passed trough a tensorflow.keras.layers.Dense layer with nparticles x nfeatures neurons and a configurable activation function. Additional hidden dense layers can be added, each with the same number of neurons and activation function. These layers learn complex patterns and represnetations in the data though learned weights and biases in order to increase the model's capabilities to capture remnant nonlinear relationships in the data After each dense layer, dropout regularization is applied to prevent overfitting and improve the generalization. The tensorflow.keras.layers.Dropout layer randomly set a fraction of input inuts to zero during the training, introducing noice and reducing reliance on specific inputs. Output Layer Finally, an output layer serves the crucial role of producing the final predictions of the model. In classification tasks, the output layer assigns probabilitiies to each class, indicating the likelihood of the input belonging to each class. Softmax activation function ensures that the predicted probabilities sum up to 1 across all classes, facilitating interpretation and decision-making. In regression tasks, the output layer directly predicts continuous values without any activation function. The model learns to map input sequences to continuous values directly, optimizing for regression objectives such as minimizing mean squared error. In practive, depending on the specified training_mode , the output layer is constructed differently. For classification tasks ( training_mode == \"classification\" ), A dense layer with nclass neurons is added, followed by a softmax activation function. For regression tasks ( training_mode == \"regression\" ), A dense layer with a single neuron is added, representing the regression output.","title":"Transformer Model"},{"location":"model/#transformer-model","text":"An implementation of a modular Transformer encoder model is available in the model.py script, where the build_transformer function builds a multi-head attention-based classifier model using TensorFlow's Keras API. The main configurable hyper-parameters are detailed above in the configuration file section . Illustration of a configurable model architecture with 2 multi-head attention layers and 3 dense layers for a 4-classes classification problem:","title":"Transformer Model"},{"location":"model/#input-layer","text":"The input layer sets the stage for passing input data into the subsequent layers of the Transformer model. A Keras input tensor is instantiated using tensorflow.keras.Input to create a placeholder tensor that is used as the input in the model. The input tensor is expetected to have a shape of (nparticles, nfeatures) , where nparticles represents the number of final state particles in the input sequence, and nfeatures represents the number of kinematic features for each particle.","title":"Input Layer"},{"location":"model/#multi-head-attention-layer","text":"The attention mechanism in multi-head attention layers is a crucial component in Transformer models. It allows a model to weigh the importance of different final state particles in the input sequence when processing each event. This mechanism enables the model to capture long-range dependencies and relationships between the final state particles. In a multi-head attention layer, the attention mechanism is applied multiple times in parallel, each with its own set of learnable parameters. This results in multiple sets of attention weights being computed simultaneously, providing the model with multiple perspectives or \"heads\" to attend to different parts of the input sequence. In the context of multi-head attention mechanisms, the input sequence is transformed into the so-called query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices which play a crucial roles in aggregating information across the input sequence. The query \\(Q\\) , key \\(K\\) and value \\(V\\) matrices are obtained by linear transformations of the input tensor \\(Q=XW_{Q}\\) , \\(K=KW_{K}\\) and \\(V=VW_{V}\\) , where \\(W_{Q}\\) , \\(W_{K}\\) and \\(W_{V}\\) are learnable weight matrices. These learnable parameters are optimized during the training process allowing the model to adapt and learn representations that are most relevant for the tast at hand. Attention scores are then computed between each pair of input sequence elements from the dot product of query and keyvectors and scaled by the square root of the dimensionality \\(d_{k}\\) . These attention scores represent the relevance or importance of other final state particles in the input sequence with respect to the current one. The attention scores are then scaled and passed through a softmax function to obtain attention weights. These weights determine how much each final state particle contributes to the representation of the current one. Finally, the weighted sum of the value vectors, where the weights are given by the attention weights, is computed. This results in a context vector for each final state particle, capturing information from other particles in the sequence based on their importance. \\(A(Q,K,V) = softmax\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\) In multi-head attention layers, this process is repeated across multiple heads, each with its own set of query, key, and value vectors in general. The output of each head is then concatenated and linearly transformed to produce the final output of the multi-head attention layer. In practice, multi-head attention layers are added to the model using the specialized Keras layers tensorflow.keras.layers.MultiHeadAttention . The number of attention heads to be uses in the mulit-head attention mechanism is specified with dedicated configurable parameter. The dimensionality of the key vector is set equal of the number of final state particles ( nparticles ). In the context of the so-called \"self-attention\" attention approach, the same input is used as the query, key and value.","title":"Multi-Head Attention Layer"},{"location":"model/#flatten-layer","text":"The flatten layer essentially collapses all dimensions except for the batch dimension, simplifying the data structure for subsequent layers. In practical terms, after passing through the multi-head attention layers, the tensorflow.keras.layers.Flatten layer reshapes the output into a linear format compatible with densely connected layers that can perform classification tasks.","title":"Flatten Layer"},{"location":"model/#dense-layer","text":"After flattening, the output of the multi-head attention layers is passed trough a tensorflow.keras.layers.Dense layer with nparticles x nfeatures neurons and a configurable activation function. Additional hidden dense layers can be added, each with the same number of neurons and activation function. These layers learn complex patterns and represnetations in the data though learned weights and biases in order to increase the model's capabilities to capture remnant nonlinear relationships in the data After each dense layer, dropout regularization is applied to prevent overfitting and improve the generalization. The tensorflow.keras.layers.Dropout layer randomly set a fraction of input inuts to zero during the training, introducing noice and reducing reliance on specific inputs.","title":"Dense Layer"},{"location":"model/#output-layer","text":"Finally, an output layer serves the crucial role of producing the final predictions of the model. In classification tasks, the output layer assigns probabilitiies to each class, indicating the likelihood of the input belonging to each class. Softmax activation function ensures that the predicted probabilities sum up to 1 across all classes, facilitating interpretation and decision-making. In regression tasks, the output layer directly predicts continuous values without any activation function. The model learns to map input sequences to continuous values directly, optimizing for regression objectives such as minimizing mean squared error. In practive, depending on the specified training_mode , the output layer is constructed differently. For classification tasks ( training_mode == \"classification\" ), A dense layer with nclass neurons is added, followed by a softmax activation function. For regression tasks ( training_mode == \"regression\" ), A dense layer with a single neuron is added, representing the regression output.","title":"Output Layer"},{"location":"preparation/","text":"Data preparation The first step is to prepare training and validation data for a dedicated project. The utils/preparation.py script, in association with the configuration file config/config.yaml , allows to simply setup the statistics and contents of the training and validation samples. usage: fst-preparation [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE Configuration file path This script serves as a comprehensive tool for preparing training datasets. Users can customize the data preparation process through a configuration file, specifying paths to data files, training modes (classification or regression), and other relevant parameters. The script then extracts particle datasets, applies normalization to ensure consistent weights, and prepares TensorFlow datasets for training. It offers options for reshuffling and batching, as well as the ability to artificially increase statistics by duplicating the dataset. The resulting datasets are saved in TensorFlow format, and the script optionally generates validation plots for further analysis. With a user-friendly command-line interface, this script streamlines the intricate process of the data preparation for the training step. Example of input feature distributions:","title":"Data Preparation"},{"location":"preparation/#data-preparation","text":"The first step is to prepare training and validation data for a dedicated project. The utils/preparation.py script, in association with the configuration file config/config.yaml , allows to simply setup the statistics and contents of the training and validation samples. usage: fst-preparation [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE Configuration file path This script serves as a comprehensive tool for preparing training datasets. Users can customize the data preparation process through a configuration file, specifying paths to data files, training modes (classification or regression), and other relevant parameters. The script then extracts particle datasets, applies normalization to ensure consistent weights, and prepares TensorFlow datasets for training. It offers options for reshuffling and batching, as well as the ability to artificially increase statistics by duplicating the dataset. The resulting datasets are saved in TensorFlow format, and the script optionally generates validation plots for further analysis. With a user-friendly command-line interface, this script streamlines the intricate process of the data preparation for the training step. Example of input feature distributions:","title":"Data preparation"},{"location":"training/","text":"Training Once the training sample are produced, the training of the Transform model can be performed by executing the utils/training.py script. usage: fst-training [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE Configuration file This script is designed for training a Transformer encoder model tailored for either classification or regression tasks within the TensorFlow ecosystem. Users provide a configuration file path specifying parameters such as the type of training (classification or regression), model architecture details, dataset locations, and training hyperparameters. The script loads configurations, builds the Transformer model accordingly, compiles it, and then trains the model using provided training and test datasets. It supports resuming training from existing checkpoints and logs various metrics, such as accuracy or mean squared error, during training. Overall, it provides a streamlined pipeline for training Transformer models with a focus on flexibility and configurability. Example of usual training curves for monitoring classification tasks:","title":"Training"},{"location":"training/#training","text":"Once the training sample are produced, the training of the Transform model can be performed by executing the utils/training.py script. usage: fst-training [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE Configuration file This script is designed for training a Transformer encoder model tailored for either classification or regression tasks within the TensorFlow ecosystem. Users provide a configuration file path specifying parameters such as the type of training (classification or regression), model architecture details, dataset locations, and training hyperparameters. The script loads configurations, builds the Transformer model accordingly, compiles it, and then trains the model using provided training and test datasets. It supports resuming training from existing checkpoints and logs various metrics, such as accuracy or mean squared error, during training. Overall, it provides a streamlined pipeline for training Transformer models with a focus on flexibility and configurability. Example of usual training curves for monitoring classification tasks:","title":"Training"},{"location":"validation/","text":"Validation Finally, the utils/validation.py script defines a few functions for validating a transformer-based machine learning model that was previously trained on a dataset. usage: fst-validation [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE Configuration file This script is dedicated to the validation step of a trained Transformer model. The script loads the trained model, performs predictions on validation datasets, and conducts various validation tasks based on user-defined options. These tasks include saving predictions, plotting model architecture, computing and visualizing confusion matrices, generating ROC curves, plotting efficiency curves, and displaying probability distributions. The script is adaptable to both classification and regression scenarios, offering different visualization and evaluation methods based on the selected training mode. The command-line interface allows users to specify the configuration file path when executing the script, contributing to ease of use and configurability. Overall, the script provides a comprehensive set of validation tools for assessing the performance of a Transformer model on new data. Confusion Matrix A confusion matrix is a key performance measurement tool which allows you to visualise, in a tabular format, the performance of a classifier by comparing predicted classes against true classes. It helps identifying which classes are being confused with each other, giving insights into areas where your model may need improvements. Beyond the various metrics which can be calculated, such as accuracy, precision, recall, specificity or F1 score, the examination of its cells can provide insights into the types of errors made by the model. For example, a high number of false positives may indicate that the model is overly sensitive and is incorrectly predicting positive outcomes too often. On the other hand, a high number of false negatives may suggest that the model is missing important patterns or features in the data. Training Performance Scores Several metrics are used in evaluating the performance of classification models. Accuracy : Accuracy is a measure of the overall correctness of a model. It is calculated as the ratio of correctly predicted instances to the total instances in the dataset: \\(A = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}.\\) Precision : Precision is the measure of the correctness of positive predictions made by the model. It is the ratio of true positive predictions to the total predicted positive instances: \\(P = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}.\\) Recall (Sensitivity) : Recall, also known as Sensitivity, is the measure of the model's ability to correctly identify positive instances out of all actual positive instances. It is calculated as the ratio of true positive predictions to the total actual positive instances: \\(R = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}.\\) F1 Score : The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall: \\(F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\\) Misclassification Difficulty : Misclassification difficulty refers to the complexity or degree of difficulty in accurately classifying instances in a dataset. It is subjective and can vary depending on factors such as class imbalance, overlap between classes, and the quality of features used for classification. Higher misclassification difficulty implies that it is more challenging for a model to correctly classify instances. Output Class Probabilities The output probability class of classification models refers to the probability assigned by the model to each class for a given input instance. In classification tasks, especially in scenarios with more than two classes (multi-class classification), the model predicts the probability distribution over all possible classes. Each class is assigned a probability score indicating the likelihood of the input instance belonging to that class. The class with the highest probability is typically chosen as the predicted class for the input instance. However, you can adjust the threshold depending on your requirements. Output probability classes are useful not only for making predictions but also for assessing the model's confidence in its predictions. Higher probabilities generally indicate higher confidence in the prediction, while lower probabilities suggest more uncertainty. LLR Discriminant The LLR (Log-Likelihood Ratio) ratio discriminant is a statistical measure used in binary classification tasks, particularly in signal detection scenarios, to distinguish between a signal and background noise. It is based on comparing the likelihoods of an observation under the signal hypothesis versus the background hypothesis. The LLR ratio discriminant represents the logarithm of the likelihood ratio of the signal probability to the sum of background probabilities for a given input instance. In signal detection problems, the model assigns probabilities to both the signal and background classes. The LLR ratio discriminant is calculated using these probabilities and is given by: \\(D = \\ln (\\frac{\\text{Signal Probability}}{\\text{Sum of Background Probabilities}})\\) A positive LLR ratio discriminant suggests that the observation is more likely to belong to the signal class, while a negative value indicates a higher likelihood of belonging to the background class. The magnitude of the LLR ratio discriminant indicates the strength of evidence for one class over the other. For example, if the LLR ratio discriminant for an observation is 2.5, it means that the observation is 10^(2.5) times more likely to belong to the signal class compared to the background class. The LLR ratio discriminant is particularly useful in scenarios where there is a need to distinguish a weak signal from background noise with high confidence. ROC Curves Receiver Operating Characteristic (ROC) curves are indispensable tools for assessing the performance of classification models, especially in scenarios where background rejection is a crucial consideration relative to signal efficiency. On the deficated validation plots, the top-right corner of the ROC space represents the optimal performance, where the model achieves high signal efficiency while maintaining high background rejection. The Area Under the ROC Curve (AUC-ROC) quantifies the overall performance of the model, with higher values indicating superior discrimination between signal and background. ROC analysis facilitates the comparison of different models' performances, helping in the selection and optimization of the model.","title":"Validation"},{"location":"validation/#validation","text":"Finally, the utils/validation.py script defines a few functions for validating a transformer-based machine learning model that was previously trained on a dataset. usage: fst-validation [-h] [--configfile CONFIGFILE] optional arguments: -h, --help Show this help message and exit --configfile CONFIGFILE Configuration file This script is dedicated to the validation step of a trained Transformer model. The script loads the trained model, performs predictions on validation datasets, and conducts various validation tasks based on user-defined options. These tasks include saving predictions, plotting model architecture, computing and visualizing confusion matrices, generating ROC curves, plotting efficiency curves, and displaying probability distributions. The script is adaptable to both classification and regression scenarios, offering different visualization and evaluation methods based on the selected training mode. The command-line interface allows users to specify the configuration file path when executing the script, contributing to ease of use and configurability. Overall, the script provides a comprehensive set of validation tools for assessing the performance of a Transformer model on new data.","title":"Validation"},{"location":"validation/#confusion-matrix","text":"A confusion matrix is a key performance measurement tool which allows you to visualise, in a tabular format, the performance of a classifier by comparing predicted classes against true classes. It helps identifying which classes are being confused with each other, giving insights into areas where your model may need improvements. Beyond the various metrics which can be calculated, such as accuracy, precision, recall, specificity or F1 score, the examination of its cells can provide insights into the types of errors made by the model. For example, a high number of false positives may indicate that the model is overly sensitive and is incorrectly predicting positive outcomes too often. On the other hand, a high number of false negatives may suggest that the model is missing important patterns or features in the data.","title":"Confusion Matrix"},{"location":"validation/#training-performance-scores","text":"Several metrics are used in evaluating the performance of classification models. Accuracy : Accuracy is a measure of the overall correctness of a model. It is calculated as the ratio of correctly predicted instances to the total instances in the dataset: \\(A = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}.\\) Precision : Precision is the measure of the correctness of positive predictions made by the model. It is the ratio of true positive predictions to the total predicted positive instances: \\(P = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}.\\) Recall (Sensitivity) : Recall, also known as Sensitivity, is the measure of the model's ability to correctly identify positive instances out of all actual positive instances. It is calculated as the ratio of true positive predictions to the total actual positive instances: \\(R = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}.\\) F1 Score : The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall: \\(F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\\) Misclassification Difficulty : Misclassification difficulty refers to the complexity or degree of difficulty in accurately classifying instances in a dataset. It is subjective and can vary depending on factors such as class imbalance, overlap between classes, and the quality of features used for classification. Higher misclassification difficulty implies that it is more challenging for a model to correctly classify instances.","title":"Training Performance Scores"},{"location":"validation/#output-class-probabilities","text":"The output probability class of classification models refers to the probability assigned by the model to each class for a given input instance. In classification tasks, especially in scenarios with more than two classes (multi-class classification), the model predicts the probability distribution over all possible classes. Each class is assigned a probability score indicating the likelihood of the input instance belonging to that class. The class with the highest probability is typically chosen as the predicted class for the input instance. However, you can adjust the threshold depending on your requirements. Output probability classes are useful not only for making predictions but also for assessing the model's confidence in its predictions. Higher probabilities generally indicate higher confidence in the prediction, while lower probabilities suggest more uncertainty.","title":"Output Class Probabilities"},{"location":"validation/#llr-discriminant","text":"The LLR (Log-Likelihood Ratio) ratio discriminant is a statistical measure used in binary classification tasks, particularly in signal detection scenarios, to distinguish between a signal and background noise. It is based on comparing the likelihoods of an observation under the signal hypothesis versus the background hypothesis. The LLR ratio discriminant represents the logarithm of the likelihood ratio of the signal probability to the sum of background probabilities for a given input instance. In signal detection problems, the model assigns probabilities to both the signal and background classes. The LLR ratio discriminant is calculated using these probabilities and is given by: \\(D = \\ln (\\frac{\\text{Signal Probability}}{\\text{Sum of Background Probabilities}})\\) A positive LLR ratio discriminant suggests that the observation is more likely to belong to the signal class, while a negative value indicates a higher likelihood of belonging to the background class. The magnitude of the LLR ratio discriminant indicates the strength of evidence for one class over the other. For example, if the LLR ratio discriminant for an observation is 2.5, it means that the observation is 10^(2.5) times more likely to belong to the signal class compared to the background class. The LLR ratio discriminant is particularly useful in scenarios where there is a need to distinguish a weak signal from background noise with high confidence.","title":"LLR Discriminant"},{"location":"validation/#roc-curves","text":"Receiver Operating Characteristic (ROC) curves are indispensable tools for assessing the performance of classification models, especially in scenarios where background rejection is a crucial consideration relative to signal efficiency. On the deficated validation plots, the top-right corner of the ROC space represents the optimal performance, where the model achieves high signal efficiency while maintaining high background rejection. The Area Under the ROC Curve (AUC-ROC) quantifies the overall performance of the model, with higher values indicating superior discrimination between signal and background. ROC analysis facilitates the comparison of different models' performances, helping in the selection and optimization of the model.","title":"ROC Curves"}]}