
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.10">
    
    
      
        <title>Transformer Model - FINAL STATE TRANSFORMER</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.472b142f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.08040f6c.min.css">
        
          
          
          <meta name="theme-color" content="#02a6f2">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="red">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformer-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="FINAL STATE TRANSFORMER" class="md-header__button md-logo" aria-label="FINAL STATE TRANSFORMER" data-md-component="logo">
      
  <img src="../img/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            FINAL STATE TRANSFORMER
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformer Model
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dev-geof/final-state-transformer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="m503.5 204.6-.7-1.8-69.7-181.78c-1.4-3.57-3.9-6.59-7.2-8.64-2.4-1.55-5.1-2.515-8-2.81-2.9-.295-5.7.083-8.4 1.11-2.7 1.02-5.1 2.66-7.1 4.78-1.9 2.12-3.3 4.67-4.1 7.44l-47 144H160.8l-47.1-144c-.8-2.77-2.2-5.31-4.1-7.43-2-2.12-4.4-3.75-7.1-4.77a18.1 18.1 0 0 0-8.38-1.113 18.4 18.4 0 0 0-8.04 2.793 18.09 18.09 0 0 0-7.16 8.64L9.267 202.8l-.724 1.8a129.57 129.57 0 0 0-3.52 82c7.747 26.9 24.047 50.7 46.447 67.6l.27.2.59.4 105.97 79.5 52.6 39.7 32 24.2c3.7 1.9 8.3 4.3 13 4.3 4.7 0 9.3-2.4 13-4.3l32-24.2 52.6-39.7 106.7-79.9.3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../code/" class="md-tabs__link">
      Code
    </a>
  </li>

      
        
  
  
    
  


  <li class="md-tabs__item">
    <a href="./" class="md-tabs__link md-tabs__link--active">
      Transformer Model
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../input/" class="md-tabs__link">
      Input Data Preparation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../training/" class="md-tabs__link">
      Training
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../validation/" class="md-tabs__link">
      Performance Assessment
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../configuration/" class="md-tabs__link">
      Configuration
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../troubleshooting/" class="md-tabs__link">
      Troubleshouting
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../copyrights/" class="md-tabs__link">
      Copyrights and Credits
    </a>
  </li>

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="FINAL STATE TRANSFORMER" class="md-nav__button md-logo" aria-label="FINAL STATE TRANSFORMER" data-md-component="logo">
      
  <img src="../img/icon.png" alt="logo">

    </a>
    FINAL STATE TRANSFORMER
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dev-geof/final-state-transformer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="m503.5 204.6-.7-1.8-69.7-181.78c-1.4-3.57-3.9-6.59-7.2-8.64-2.4-1.55-5.1-2.515-8-2.81-2.9-.295-5.7.083-8.4 1.11-2.7 1.02-5.1 2.66-7.1 4.78-1.9 2.12-3.3 4.67-4.1 7.44l-47 144H160.8l-47.1-144c-.8-2.77-2.2-5.31-4.1-7.43-2-2.12-4.4-3.75-7.1-4.77a18.1 18.1 0 0 0-8.38-1.113 18.4 18.4 0 0 0-8.04 2.793 18.09 18.09 0 0 0-7.16 8.64L9.267 202.8l-.724 1.8a129.57 129.57 0 0 0-3.52 82c7.747 26.9 24.047 50.7 46.447 67.6l.27.2.59.4 105.97 79.5 52.6 39.7 32 24.2c3.7 1.9 8.3 4.3 13 4.3 4.7 0 9.3-2.4 13-4.3l32-24.2 52.6-39.7 106.7-79.9.3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../code/" class="md-nav__link">
        Code
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Transformer Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Transformer Model
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#input-layer" class="md-nav__link">
    Input Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#embedding-layer" class="md-nav__link">
    Embedding Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-head-attention-layer" class="md-nav__link">
    Multi-Head Attention Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#flatten-layer" class="md-nav__link">
    Flatten Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dense-layer" class="md-nav__link">
    Dense Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-layer" class="md-nav__link">
    Output Layer
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../input/" class="md-nav__link">
        Input Data Preparation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../training/" class="md-nav__link">
        Training
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../validation/" class="md-nav__link">
        Performance Assessment
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../configuration/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../troubleshooting/" class="md-nav__link">
        Troubleshouting
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../copyrights/" class="md-nav__link">
        Copyrights and Credits
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#input-layer" class="md-nav__link">
    Input Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#embedding-layer" class="md-nav__link">
    Embedding Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-head-attention-layer" class="md-nav__link">
    Multi-Head Attention Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#flatten-layer" class="md-nav__link">
    Flatten Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dense-layer" class="md-nav__link">
    Dense Layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-layer" class="md-nav__link">
    Output Layer
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/dev-geof/final-state-transformer/edit/master/docs/model.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="transformer-model">Transformer Model</h1>
<p>An implementation of a modular Transformer encoder model is available in the model.py script, where the <code>build_transformer</code> function builds a multi-head attention-based classifier model using TensorFlow’s Keras API.</p>
<p>Illustration of a configurable model architecture with 1 embedding layer, 2 multi-head attention layers and 1 dense layer for a 4-classes classification problem:   </p>
<p><img alt="Example of Model Architecture" src="../img/model_example_3d.png" /></p>
<h2 id="input-layer">Input Layer</h2>
<p>The input layer sets the stage for passing input data into the subsequent layers of the Transformer model. A Keras input tensor is instantiated using <code>tensorflow.keras.Input</code> to create a placeholder tensor that is used as the input in the model. The input tensor is expetected to have a shape of <code>(nparticles, nfeatures)</code>, where <code>nparticles</code> represents the number of final state particles in the input sequence, and <code>nfeatures</code> represents the number of kinematic features for each particle.</p>
<h2 id="embedding-layer">Embedding Layer</h2>
<p>An optional embedding layer can be instrumental for eleviating performance of the network. For instance it can be used for transforming categorical or discrete input features into dense, continuous vector representations that can be learned during training. For tasks that require a better data representation than the input one, such as unfolding the input space to reveal intricate information that the network can better utilize, the embedding layer also proves to be invaluable, enabling the model to uncover and effectively utilize nuanced patterns and relationships within the input data. By learning the embeddings from the input data, the model can adapt to the specific task at hand, ultimately enhancing its ability to extract meaningful features and make accurate predictions. </p>
<p>A custom <code>FloatEmbedding</code> class initializes embedding weights in its constructor, which are randomly initialized and trainable, and then performs the embedding lookup operation in its call method. The embedding layer is optionally included based on the <code>embedding</code> parameter, allowing for the transformation of input. The <code>embedding_dim</code> parameter defines the dimension of the embedded space for which the data from input feature space is project into. During the training process, the model learns the optimal transformation matrix built upon the trainable parameters. Purely linear transformation is considered so far, no additional activation function is included.</p>
<h2 id="multi-head-attention-layer">Multi-Head Attention Layer</h2>
<p>The attention mechanism in multi-head attention layers is a crucial component in Transformer models. It allows a model to weigh the importance of different final state particles in the input sequence when processing each event. This mechanism enables the model to capture long-range dependencies and relationships between the final state particles. In a multi-head attention layer, the attention mechanism is applied multiple times in parallel, each with its own set of learnable parameters. This results in multiple sets of attention weights being computed simultaneously, providing the model with multiple perspectives or "heads" to attend to different parts of the input sequence.</p>
<p>In the context of multi-head attention mechanisms, the input sequence is transformed into the so-called query <span class="arithmatex">\(Q\)</span>, key <span class="arithmatex">\(K\)</span> and value <span class="arithmatex">\(V\)</span> matrices which play a crucial roles in aggregating information across the input sequence. The query <span class="arithmatex">\(Q\)</span>, key <span class="arithmatex">\(K\)</span> and value <span class="arithmatex">\(V\)</span> matrices are obtained by linear transformations of the input tensor <span class="arithmatex">\(Q=XW_{Q}\)</span>, <span class="arithmatex">\(K=KW_{K}\)</span> and <span class="arithmatex">\(V=VW_{V}\)</span>, where <span class="arithmatex">\(W_{Q}\)</span>, <span class="arithmatex">\(W_{K}\)</span> and <span class="arithmatex">\(W_{V}\)</span> are learnable weight matrices. These learnable parameters are optimized during the training process allowing the model to adapt and learn representations that are most relevant for the tast at hand. </p>
<p>Attention scores are then computed between each pair of input sequence elements from the dot product of query and keyvectors and scaled by the square root of the dimensionality <span class="arithmatex">\(d_{k}\)</span>. These attention scores represent the relevance or importance of other final state particles in the input sequence with respect to the current one. The attention scores are then scaled and passed through a softmax function to obtain attention weights. These weights determine how much each final state particle contributes to the representation of the current one. Finally, the weighted sum of the value vectors, where the weights are given by the attention weights, is computed. This results in a context vector for each final state particle, capturing information from other particles in the sequence based on their importance.</p>
<p><span class="arithmatex">\(A(Q,K,V) = softmax\left( \frac{QK^{T}}{\sqrt{d_{k}}} \right)V\)</span></p>
<p>In multi-head attention layers, this process is repeated across multiple heads, each with its own set of query, key, and value vectors in general. The output of each head is then concatenated and linearly transformed to produce the final output of the multi-head attention layer.</p>
<p>In practice, multi-head attention layers are added to the model using the specialized Keras layers <code>tensorflow.keras.layers.MultiHeadAttention</code>. The number of attention heads to be uses in the mulit-head attention mechanism is specified with dedicated configurable parameter. The dimensionality of the key vector is set equal of the number of final state particles (<code>nparticles</code>). In the context of the so-called "self-attention" attention approach, the same input is used as the query, key and value. </p>
<h2 id="flatten-layer">Flatten Layer</h2>
<p>The flatten layer essentially collapses all dimensions except for the batch dimension, simplifying the data structure for subsequent layers. In practical terms, after passing through the multi-head attention layers, the <code>tensorflow.keras.layers.Flatten</code> layer reshapes the output into a linear format compatible with densely connected layers that can perform classification tasks.</p>
<h2 id="dense-layer">Dense Layer</h2>
<p>After flattening, the output of the multi-head attention layers is passed trough a <code>tensorflow.keras.layers.Dense</code> layer with <code>nparticles x nfeatures</code> neurons and a configurable activation function. Additional hidden dense layers can be added, each with the same number of neurons and activation function. These layers learn complex patterns and represnetations in the data though learned weights and biases in order to increase the model's capabilities to capture remnant nonlinear relationships in the data</p>
<p>After each dense layer, dropout regularization is applied to prevent overfitting and improve the generalization. The <code>tensorflow.keras.layers.Dropout</code> layer randomly set a fraction of input inuts to zero during the training, introducing noice and reducing reliance on specific inputs. </p>
<h2 id="output-layer">Output Layer</h2>
<p>Finally, an output layer serves the crucial role of producing the final predictions of the model. In classification tasks, the output layer assigns probabilitiies to each class, indicating the likelihood of the input belonging to each class. Softmax activation function ensures that the predicted probabilities sum up to 1 across all classes, facilitating interpretation and decision-making. In regression tasks, the output layer directly predicts continuous values without any activation function. The model learns to map input sequences to continuous values directly, optimizing for regression objectives such as minimizing mean squared error. </p>
<p>In practive, depending on the specified <code>training_mode</code>, the output layer is constructed differently.
For classification tasks (<code>training_mode == "classification"</code>), A dense layer with <code>nclass</code> neurons is added, followed by a softmax activation function. For regression tasks (<code>training_mode == "regression"</code>), A dense layer with a single neuron is added, representing the regression output.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../code/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Code" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Code
            </div>
          </div>
        </a>
      
      
        
        <a href="../input/" class="md-footer__link md-footer__link--next" aria-label="Next: Input Data Preparation" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Input Data Preparation
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.footer", "navigation.tabs", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>